{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.fft\n",
    "\n",
    "from radon import get_operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu' #'cuda' for GPU, 'cpu' otherwise\n",
    "radon_op, fbp_op = get_operators(n_angles=32, image_size=256, circle=True, device=device)\n",
    "\n",
    "#forward model representing a radon transform\n",
    "def A(x):\n",
    "    y = radon_op(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f139864-e71f-48ae-a5f0-67818154c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray()\n",
    "img = torch.load(data_path+'0')\n",
    "plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f944e-7cd8-4c4c-b983-f51cbbdc3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinogram = A(img.unsqueeze(1))  # unsqueeze because A expects a channel dimension at location 1\n",
    "print(sinogram.shape)\n",
    "plt.imshow(sinogram.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb48a0f-4d45-4b43-bead-5a0252566a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use filtered back projection as an estimate of A.T\n",
    "def AT(y):\n",
    "    x=fbp_op(y)\n",
    "    x = x.to(torch.float32)\n",
    "    return x\n",
    "\n",
    "reconstruction = AT(sinogram)\n",
    "plt.imshow(reconstruction.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8001bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataset that returns sinograms and ground-truth images\n",
    "class CTDataSet(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.path))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.load(self.path+str(idx)).to(device)\n",
    "        sino = A(img.unsqueeze(1))\n",
    "        return img.squeeze(), sino.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcbc547",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = CTDataSet(data_path)\n",
    "\n",
    "#Split dataset into 99 imgs for training and 1 img for validation\n",
    "train_set, val_set = torch.utils.data.random_split(data_set,[99,1],generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "#Dataloader\n",
    "train_dl = DataLoader(train_set, batch_size=1)\n",
    "val_dl = DataLoader(val_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in code here\n",
    "# you are free to create as many classes as you need\n",
    "\n",
    "\n",
    "\n",
    "#Whole Variational Network    \n",
    "class VarNet(nn.Module):\n",
    "    def __init__(self, num_cascades=5):\n",
    "        super(VarNet, self).__init__()\n",
    "        \n",
    "        # fill in code here\n",
    "        \n",
    "    def forward(self, y):\n",
    "        \n",
    "        # fill in code here\n",
    "         \n",
    "        return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c26e22ad",
   "metadata": {},
   "source": [
    "### Below are funtions to train and test the network\n",
    "\n",
    "You can modify the functions or the hyperparameters for training as you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loss\n",
    "def mse(gt: torch.Tensor, pred:torch.Tensor)-> torch.Tensor:\n",
    "    loss = torch.nn.MSELoss()\n",
    "    return loss(gt,pred)\n",
    "\n",
    "#train function\n",
    "def train_step(model, optimizer, dataloader_sample):\n",
    "    model.train()\n",
    "    \n",
    "    # reset optimizer's gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # define variables\n",
    "    x, y = dataloader_sample\n",
    "      \n",
    "    # get the prediction\n",
    "    pred = model(y.unsqueeze(1))[-1].squeeze(1)\n",
    "    pred_loss = mse(pred, x)\n",
    "    \n",
    "    #one step of training\n",
    "    pred_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return pred_loss.item()\n",
    "\n",
    "#test function\n",
    "def validation_step(model, dataloader_sample): \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        \n",
    "        x, y = dataloader_sample\n",
    "        \n",
    "        # get the prediction\n",
    "        pred = model(y.unsqueeze(1))[-1].squeeze(1)\n",
    "        pred_loss = mse(pred, x)\n",
    "\n",
    "    return pred_loss.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dd7d084",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "As optimizer, we choose the Adam optimizer (a standard adaptive gradient method). We then train the model for 10 epochs; training for more epochs gives better results, but after 10 epochs we already get a model that works reasonably well for image recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b26e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0)\n",
    "\n",
    "max_epoch = 10\n",
    "mse_train=[]\n",
    "mse_val= []\n",
    "\n",
    "for epoch in tqdm(range(max_epoch)):\n",
    "    # Initialize Loss and Accuracy\n",
    "    train_loss = val_loss= 0.0\n",
    "    \n",
    "    ### Training Phase\n",
    "    \n",
    "    ## Iterate over the train_dataloader\n",
    "    with tqdm(total=len(train_dl)) as pbar:\n",
    "        for sample in train_dl:            \n",
    "            curr_loss = train_step(model, optimizer, sample)\n",
    "            train_loss += curr_loss / len(train_dl) \n",
    "            pbar.update(1)\n",
    "    \n",
    "    mse_train.append(train_loss)\n",
    "\n",
    "\n",
    "    ### Validation Phase \n",
    "    \n",
    "    ## Validation_dataloader\n",
    "    with tqdm(total=len(val_dl)) as pbar:\n",
    "        for sample in val_dl: \n",
    "            curr_loss = validation_step(model, sample)\n",
    "            val_loss += curr_loss / len(val_dl)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    mse_val.append(val_loss) \n",
    "    \n",
    "    print(epoch, train_loss, val_loss)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_val)\n",
    "plt.title('Validation Error')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e316fd3b",
   "metadata": {},
   "source": [
    "### Visualization how VarNet reconstructs an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=next(iter(val_dl))  #img from val set\n",
    "\n",
    "#reconstruction\n",
    "with torch.no_grad(): \n",
    "    x, y = sample\n",
    "    pred = model(y.unsqueeze(1))\n",
    "    fbp = AT(y.unsqueeze(1))\n",
    "\n",
    "plt.gray()\n",
    "fig, ax = plt.subplots(1, 7,figsize=(15, 15))\n",
    "\n",
    "ax[0].imshow(fbp.squeeze().cpu())\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].set_title('FBP') \n",
    "\n",
    "for i in range(5):\n",
    "    ax[i+1].imshow(pred[i].squeeze().cpu())\n",
    "    ax[i+1].set_xticks([])\n",
    "    ax[i+1].set_yticks([])\n",
    "    ax[i+1].set_title('Cascade '+ str(i+1))\n",
    "    \n",
    "ax[6].imshow(x.squeeze().cpu())   \n",
    "ax[6].set_xticks([])\n",
    "ax[6].set_yticks([])\n",
    "ax[6].set_title('Ground Truth')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef104f-69ec-4b9a-b91a-cb78ac32ac52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
